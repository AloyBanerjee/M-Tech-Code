{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9aws0meaRBS"
   },
   "source": [
    "#### Submission Deadline: 31/05/2023  -  23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2OUn001UbEl"
   },
   "source": [
    "# Question: 01\n",
    "\n",
    "**MAP REDUCE - DATASET:** write map reduce program(s) using python-mr in colab notebook and complete the following exercises\n",
    "\n",
    "  - Find the files which contain the following phrases\n",
    "    - “draping the folds of her velvet dress”\n",
    "    - “grim clasp to the iron stanchions”\n",
    "    - “Lo, I teach you the Superman!”\n",
    "    - “Through that divine allegiance”\n",
    "  - Find the file which has maximum number of tokens\n",
    "  - Find the file which has the longest sentence (a sentence ends in a full stop and can span multiple lines)\n",
    "\n",
    "\n",
    "**Dataset:** \n",
    "\n",
    "  - [pg1513.txt](https://www.gutenberg.org/cache/epub/1513/pg1513.txt)<br>\n",
    "  - [2160-0.txt](https://www.gutenberg.org/files/2160/2160-0.txt)<br>\n",
    "  - [pg11.txt](https://www.gutenberg.org/cache/epub/11/pg11.txt)<br>\n",
    "  - [pg2600.txt](https://www.gutenberg.org/cache/epub/2600/pg2600.txt)<br>\n",
    "  - [345.txt.utf-8](https://www.gutenberg.org/ebooks/345.txt.utf-8)<br>\n",
    "  - [64317.txt.utf-8](https://www.gutenberg.org/ebooks/64317.txt.utf-8)<br>\n",
    "  - [70815.txt.utf-8](https://www.gutenberg.org/ebooks/70815.txt.utf-8)<br>\n",
    "  - [1080-0.txt](https://www.gutenberg.org/files/1080/1080-0.txt)<br>\n",
    "  - [70817.txt.utf-8](https://www.gutenberg.org/ebooks/70817.txt.utf-8)<br>\n",
    "  - [1998.txt.utf-8](https://www.gutenberg.org/ebooks/1998.txt.utf-8)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtmETPwNvEmO"
   },
   "source": [
    "## Answer: 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting URL\n",
    "#### https://mrjob.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing common library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPath = 'C:/Users/Admin/Coding-M-Tech/AI Lab 2 - ID5003W/Assignment 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68OcLN0-vEmO",
    "outputId": "e8845623-1c56-4494-b2ed-a05e02eab996"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TMS5Yx_BwiuE"
   },
   "outputs": [],
   "source": [
    "os.chdir(workingPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD5rsVyRvEmQ"
   },
   "source": [
    "#### Common Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TN6n09BovEmQ"
   },
   "outputs": [],
   "source": [
    "## All the downloadable url\n",
    "url_pg1513 = 'https://www.gutenberg.org/cache/epub/1513/pg1513.txt'\n",
    "url_2160 = 'https://www.gutenberg.org/files/2160/2160-0.txt'\n",
    "url_pg11 = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "url_pg2600 = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "url_pg345 = 'https://www.gutenberg.org/cache/epub/345/pg345.txt'\n",
    "url_pg64317 = 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'\n",
    "url_pg70815 = 'https://www.gutenberg.org/cache/epub/70815/pg70815.txt'\n",
    "url_1080 = 'https://www.gutenberg.org/files/1080/1080-0.txt'\n",
    "url_pg70817 = 'https://www.gutenberg.org/cache/epub/70817/pg70817.txt'\n",
    "url_pg1998 = 'https://www.gutenberg.org/cache/epub/1998/pg1998.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYTHCbyMvEmR"
   },
   "source": [
    "#### Map reduce Functionality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAD9uvtFAFcW"
   },
   "source": [
    "###### Phrase Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i53ZaAhZvEmR",
    "outputId": "c9ba5dcd-c592-42b1-9b01-89312d107ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting phrase_search.py\n"
     ]
    }
   ],
   "source": [
    "%%file phrase_search.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os\n",
    "\n",
    "class FindPhrase(MRJob):\n",
    "     \n",
    "    def configure_args(self):\n",
    "        super(FindPhrase, self).configure_args()\n",
    "        self.add_passthru_arg('--phrase', type=str, help='Phrase to find in files')\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.phrase = self.options.phrase\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        filename = os.getenv('mapreduce_map_input_file')\n",
    "        file_name = filename.split('/')[-1]\n",
    "        if self.phrase in line:\n",
    "            yield (self.phrase,file_name), 1\n",
    "\n",
    "    def reducer(self, key, values): \n",
    "        yield key, f\"{sum(values)} occurance of : {key[0]} phrase, is present in : {key[1]} file\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FindPhrase.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qrQKqRbAbHG"
   },
   "source": [
    "#### Max Token Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N-xQlaB_AheN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting max_token_search.py\n"
     ]
    }
   ],
   "source": [
    "%%file max_token_search.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os\n",
    "\n",
    "class FindMaxToken(MRJob):\n",
    "     \n",
    "    def mapper(self, _, line):\n",
    "        filename = os.getenv('mapreduce_map_input_file')\n",
    "        tokens = line.strip().split()\n",
    "        file_name = filename.split('/')[-1]\n",
    "        yield file_name, len(tokens)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, f\" file has token value of : {sum(values) }\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FindMaxToken.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Length Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting max_length_search.py\n"
     ]
    }
   ],
   "source": [
    "%%file max_length_search.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os\n",
    "import re\n",
    "\n",
    "class FindMaxSentenceLength(MRJob):\n",
    "     \n",
    "    def mapper(self, _, line):\n",
    "        filename = os.getenv('mapreduce_map_input_file')\n",
    "        sentences = re.split(r'(?<=[.])\\s+', line)\n",
    "        file_name = filename.split('/')[-1]\n",
    "        for sentence in sentences:\n",
    "            yield file_name, len(sentence)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, f\" file has sentence of maximum {max(values)} length\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FindMaxSentenceLength.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OpbXGP1vEmR"
   },
   "source": [
    "#### Creating a dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bYCqLlnbvEmS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file Q1-Dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir Q1-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7WYcHHsvEmS"
   },
   "source": [
    "#### Downloading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yskRVmIbvEmS",
    "outputId": "00c22e1c-e9a4-4b23-dd40-df72f696d8be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Q1-Dataset/pg1998.txt', <http.client.HTTPMessage at 0x2a6bc0cbd00>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(url_pg1513,'Q1-Dataset/pg1513.txt')\n",
    "urllib.request.urlretrieve(url_2160,'Q1-Dataset/2160-0.txt')\n",
    "urllib.request.urlretrieve(url_pg11,'Q1-Dataset/pg11.txt')\n",
    "urllib.request.urlretrieve(url_pg2600,'Q1-Dataset/pg2600.txt')\n",
    "urllib.request.urlretrieve(url_pg345,'Q1-Dataset/pg345.txt')\n",
    "urllib.request.urlretrieve(url_pg64317,'Q1-Dataset/pg64317.txt')\n",
    "urllib.request.urlretrieve(url_pg70815,'Q1-Dataset/pg70815.txt')\n",
    "urllib.request.urlretrieve(url_1080,'Q1-Dataset/1080-0.txt')\n",
    "urllib.request.urlretrieve(url_pg70817,'Q1-Dataset/pg70817.txt')\n",
    "urllib.request.urlretrieve(url_pg1998,'Q1-Dataset/pg1998.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-ZNv-cmvEmS"
   },
   "source": [
    "##### 1a. Running Phrase Finder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_-PPi_gvEmS",
    "outputId": "e027720b-d03f-4c98-a9f1-23f9e7fb9023"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144457.412711\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144457.412711\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144457.412711\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144457.412711...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"draping the folds of her velvet dress\",\"pg2600.txt\"]\t\"1 occurance of : draping the folds of her velvet dress phrase, is present in : pg2600.txt file\"\n",
      "[\"grim clasp to the iron stanchions\",\"pg345.txt\"]\t\"1 occurance of : grim clasp to the iron stanchions phrase, is present in : pg345.txt file\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144459.316365\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144459.316365\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144459.316365\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144459.316365...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Lo, I teach you the Superman!\",\"pg1998.txt\"]\t\"1 occurance of : Lo, I teach you the Superman! phrase, is present in : pg1998.txt file\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144501.191245\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144501.191245\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144501.191245\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144501.191245...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144503.058394\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144503.058394\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144503.058394\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\phrase_search.Admin.20230527.144503.058394...\n"
     ]
    }
   ],
   "source": [
    "!python phrase_search.py --phrase=\"draping the folds of her velvet dress\" \"Q1-Dataset/\"\n",
    "\n",
    "!python phrase_search.py --phrase=\"grim clasp to the iron stanchions\" \"Q1-Dataset/\"\n",
    "\n",
    "!python phrase_search.py --phrase=\"Lo, I teach you the Superman!\" \"Q1-Dataset/\"\n",
    "\n",
    "!python phrase_search.py --phrase=\"Through that divine allegiance\" \"Q1-Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 1a .\n",
    "#### \"draping the folds of her velvet dress\" phrase is available in pg2600.txt file\n",
    "#### \"grim clasp to the iron stanchions\"   phrase is available in pg345.txt file\n",
    "#### \"Lo, I teach you the Superman!\"  phrase is available in 1998.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1fbAyu7vEmS",
    "outputId": "4825daee-7079-41cd-a441-22de0eedec72"
   },
   "source": [
    "##### 1b. Running Max Token Finder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YYDF22jqvEmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Q1-Dataset\\\\1080-0.txt\"\t\" file has token value of : 6512\"\n",
      "\"Q1-Dataset\\\\2160-0.txt\"\t\" file has token value of : 151379\"\n",
      "\"Q1-Dataset\\\\pg11.txt\"\t\" file has token value of : 29590\"\n",
      "\"Q1-Dataset\\\\pg1513.txt\"\t\" file has token value of : 29002\"\n",
      "\"Q1-Dataset\\\\pg1998.txt\"\t\" file has token value of : 113997\"\n",
      "\"Q1-Dataset\\\\pg2600.txt\"\t\" file has token value of : 566330\"\n",
      "\"Q1-Dataset\\\\pg345.txt\"\t\" file has token value of : 164459\"\n",
      "\"Q1-Dataset\\\\pg64317.txt\"\t\" file has token value of : 51257\"\n",
      "\"Q1-Dataset\\\\pg70815.txt\"\t\" file has token value of : 82782\"\n",
      "\"Q1-Dataset\\\\pg70817.txt\"\t\" file has token value of : 28184\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\max_token_search.Admin.20230527.144522.041254\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\max_token_search.Admin.20230527.144522.041254\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\max_token_search.Admin.20230527.144522.041254\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\max_token_search.Admin.20230527.144522.041254...\n"
     ]
    }
   ],
   "source": [
    "!python max_token_search.py \"Q1-Dataset/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOYf9bgpvEmT"
   },
   "source": [
    "### Conclusion 1b. \n",
    "#### pg2600.txt has maximum number of token i.e. 566330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NtZA_efvEmT"
   },
   "source": [
    "##### 1c. Running Find Max Sentence Length method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CfW3-mMWvEmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Q1-Dataset\\\\1080-0.txt\"\t\" file has sentence of maximum 72 length\"\n",
      "\"Q1-Dataset\\\\2160-0.txt\"\t\" file has sentence of maximum 79 length\"\n",
      "\"Q1-Dataset\\\\pg11.txt\"\t\" file has sentence of maximum 82 length\"\n",
      "\"Q1-Dataset\\\\pg1513.txt\"\t\" file has sentence of maximum 90 length\"\n",
      "\"Q1-Dataset\\\\pg1998.txt\"\t\" file has sentence of maximum 78 length\"\n",
      "\"Q1-Dataset\\\\pg2600.txt\"\t\" file has sentence of maximum 74 length\"\n",
      "\"Q1-Dataset\\\\pg345.txt\"\t\" file has sentence of maximum 73 length\"\n",
      "\"Q1-Dataset\\\\pg64317.txt\"\t\" file has sentence of maximum 72 length\"\n",
      "\"Q1-Dataset\\\\pg70815.txt\"\t\" file has sentence of maximum 72 length\"\n",
      "\"Q1-Dataset\\\\pg70817.txt\"\t\" file has sentence of maximum 72 length\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\max_length_search.Admin.20230527.144529.496349\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\Admin\\AppData\\Local\\Temp\\max_length_search.Admin.20230527.144529.496349\\output\n",
      "Streaming final output from C:\\Users\\Admin\\AppData\\Local\\Temp\\max_length_search.Admin.20230527.144529.496349\\output...\n",
      "Removing temp directory C:\\Users\\Admin\\AppData\\Local\\Temp\\max_length_search.Admin.20230527.144529.496349...\n"
     ]
    }
   ],
   "source": [
    "!python max_length_search.py \"Q1-Dataset/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 1c. \n",
    "#### pg1513.txt has maximum sentence length of 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : \n",
    "\n",
    "### Q. Find the files which contain the following phrases\n",
    "### A. \"draping the folds of her velvet dress\" phrase is available in pg2600.txt file\n",
    "### \"grim clasp to the iron stanchions\"   phrase is available in pg345.txt file\n",
    "### \"Lo, I teach you the Superman!\"  phrase is available in 1998.txt file\n",
    "\n",
    "\n",
    "### Q. Find the file which has maximum number of tokens\n",
    "### A. pg2600.txt has maximum number of token i.e. 566330\n",
    "\n",
    "\n",
    "### Q. Find the file which has the longest sentence (a sentence ends in a full stop and can span multiple lines)\n",
    "### A. pg1513.txt has maximum sentence length of 90.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ESNEl4evEmT"
   },
   "source": [
    "# Question: 02\n",
    "\n",
    "**Write a PySpark RDD program to compute:**\n",
    "\n",
    "\n",
    "  - The list of common (overlapping / intersection ) words across all the ten files listed above.\n",
    "  - List the word frequency of common words across files as a dict ( tip: use join method in the rdd). The size of the dict equals the list of common words and each key in the top level dict contains the word count corresponding to all the ten files\n",
    "\n",
    "    - {“word”: {“pg1513.txt”:10,”2160-0.txt”:25,... } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ3iIh7wvEmT"
   },
   "source": [
    "## Answer: 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 5) (Mou-Aloy.mshome.net executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rdd \u001b[38;5;129;01min\u001b[39;00m rdd_list[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     20\u001b[0m     common_words_rdd \u001b[38;5;241m=\u001b[39m common_words_rdd\u001b[38;5;241m.\u001b[39mintersection(rdd)\n\u001b[1;32m---> 22\u001b[0m common_words \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_words_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommon Words are:\u001b[39m\u001b[38;5;124m\"\u001b[39m, common_words)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Now process each file to get word frequencies\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 5) (Mou-Aloy.mshome.net executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setMaster('local').setAppName('CommonWords')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# List of files\n",
    "files = ['Q1-Dataset/pg1513.txt','Q1-Dataset/2160-0.txt','Q1-Dataset/pg11.txt','Q1-Dataset/pg2600.txt','Q1-Dataset/pg345.txt','Q1-Dataset/pg64317.txt','Q1-Dataset/pg70815.txt','Q1-Dataset/1080-0.txt','Q1-Dataset/pg70817.txt','Q1-Dataset/pg1998.txt']\n",
    "# Process each file to get RDDs of words\n",
    "rdd_list = []\n",
    "for file in files:\n",
    "    rdd = sc.textFile(file)\n",
    "    words = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "    rdd_list.append(words)\n",
    "\n",
    "# Compute intersection of all files to get common words\n",
    "common_words_rdd = rdd_list[0]\n",
    "for rdd in rdd_list[1:]:\n",
    "    common_words_rdd = common_words_rdd.intersection(rdd)\n",
    "\n",
    "common_words = common_words_rdd.distinct().collect()\n",
    "\n",
    "print(\"Common Words are:\", common_words)\n",
    "\n",
    "# Now process each file to get word frequencies\n",
    "freq_dict = {}\n",
    "for file in files:\n",
    "    rdd = sc.textFile(file)\n",
    "    words = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "    word_freq = words.countByValue()\n",
    "    for word in common_words:\n",
    "        if word in word_freq:\n",
    "            if word not in freq_dict:\n",
    "                freq_dict[word] = {}\n",
    "            freq_dict[word][file] = word_freq[word]\n",
    "\n",
    "print(freq_dict)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZXBV49fvEmU"
   },
   "source": [
    "# Question: 03\n",
    "\n",
    "Perform an exploratory data analysis on the primary_data.csv of the Mushroom dataset [Secondary Mushroom Dataset](https://archive.ics.uci.edu/ml/datasets/Secondary+Mushroom+Dataset) using pandas and answer the following questions:\n",
    "  - How many mushrooms are poisonous?\n",
    "  - Which family contains most poisonous mushrooms?\n",
    "  - Do all poisonous mushrooms have a ring?\n",
    "  - Which seasons do poisonous mushrooms grow typically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBGIFsUpvEmU"
   },
   "source": [
    "## Answer: 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AAJ78UavEmU"
   },
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9_XiGanvEmU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0asaLAyvEmU"
   },
   "source": [
    "#### Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7-aeHGyvEmU"
   },
   "outputs": [],
   "source": [
    "working_dir_path = 'C:/Users/Admin/Coding-M-Tech/AI Lab 2 - ID5003W/Assignment 1/Q3-Dataset'\n",
    "primaryFileName = 'primary_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haB6LrykvEmU"
   },
   "source": [
    "#### Change the folder location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwH32Z5HvEmV"
   },
   "outputs": [],
   "source": [
    "os.chdir(working_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeCmkaK4vEmV"
   },
   "source": [
    "#### Loading primary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRXEXS4ovEmV"
   },
   "outputs": [],
   "source": [
    "df_primary_data = pd.read_csv(primaryFileName,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "-N3phpkmvEmV",
    "outputId": "1b6346f0-a570-415b-8008-c6c3fc298ad4"
   },
   "outputs": [],
   "source": [
    "df_primary_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAha1JQAvEmV"
   },
   "source": [
    "#### Perform basic statistical analysis and missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "8yo68r2UvEmV",
    "outputId": "83c4eb4f-6964-4c51-803c-a50ded5e3f15"
   },
   "outputs": [],
   "source": [
    "df_primary_data.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THPa79J4vEmW",
    "outputId": "00863537-9b7f-4820-b1b9-4e601c354a0f"
   },
   "outputs": [],
   "source": [
    "df_primary_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNfz4NAmvEmW",
    "outputId": "edc827ff-daeb-445f-ab47-48acb7f4c04f"
   },
   "outputs": [],
   "source": [
    "df_primary_data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q3nWM90vEmm"
   },
   "outputs": [],
   "source": [
    "poisonous_mushroom = df_primary_data[df_primary_data['class'] == 'p']\n",
    "edible_mushroom = df_primary_data[df_primary_data['class'] == 'e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUvh0jHxvEmm"
   },
   "source": [
    "##### Graphical representation of the distribution of poisonous and edible mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "EobbXrRVvEmm",
    "outputId": "eb46f88d-e00c-4c0a-c17a-47334fc8bcef",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('==========Distribution of poisonous and edible mushroom============')\n",
    "mushroom_type_count= df_primary_data['class'].value_counts()  \n",
    "sns.barplot(x=mushroom_type_count.index, y=mushroom_type_count.values)\n",
    "plt.ylabel('Number of data', fontsize=9)\n",
    "plt.xlabel('class', fontsize=9)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();\n",
    "df_primary_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGvXSj4hvEmm"
   },
   "source": [
    "### Q. How many mushrooms are poisonous?\n",
    "### A. 96 mushrooms are poisonous and 77 are edible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr63AgM2vEmm"
   },
   "source": [
    "##### Graphical representation of the distribution of poisonous mushroom among different family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "VNlxnaguvEmn",
    "outputId": "4d2d001c-e0c9-4524-ad26-b39f56a18637"
   },
   "outputs": [],
   "source": [
    "poisonous_mushroom_type_count= poisonous_mushroom['family'].value_counts()  \n",
    "sns.barplot(x=poisonous_mushroom_type_count.index, y=poisonous_mushroom_type_count.values)\n",
    "plt.ylabel('Number of data', fontsize=9)\n",
    "plt.xlabel('Family', fontsize=9)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();\n",
    "df_primary_data['family'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCamM6lZvEmn"
   },
   "source": [
    "### Q. Which family contains most poisonous mushrooms?\n",
    "### A. Tricholoma, mushroom family has highest number of poisonous mushrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK6ynVFQvEmn"
   },
   "source": [
    "##### Analysis of poisonous mushroom based on its ring characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8b2yjTXvEmn",
    "outputId": "43997db9-591c-477c-8ee0-346d5cfeb126"
   },
   "outputs": [],
   "source": [
    "np.unique(poisonous_mushroom['has-ring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy7ZaybdvEmn"
   },
   "source": [
    "##### It has been observed that the unique value of mushroom has-ring column is [f] & [t], which clearly indicate that only ring exists among the mushroom which has 'has-ring' column value as [t], so we will filter that out and get the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FShBHmwXvEmo"
   },
   "outputs": [],
   "source": [
    "ring_poisonous_mushroom = poisonous_mushroom[poisonous_mushroom['has-ring'] != '[t]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "_7IfqYl2vEmo",
    "outputId": "eb1a312c-249d-44c1-e305-e314a767cbbb"
   },
   "outputs": [],
   "source": [
    "ring_poisonous_mushroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urQoXR_UvEmo"
   },
   "source": [
    "### Q. Do all poisonous mushrooms have a ring?\n",
    "### A. No, all the poisonous mushroom does not have ring, only 70 out of 96 poisonous mushroom have ring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsavrrYhvEmo"
   },
   "source": [
    "##### Analysis of poisonous mushroom based on the season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlnitFIBvEmo",
    "outputId": "eb1b9057-01c1-4e78-faf8-1cfee9fc9a4e"
   },
   "outputs": [],
   "source": [
    "np.unique(poisonous_mushroom['season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmv4OaGvEmp"
   },
   "source": [
    "##### Graphical representation of the distribution of poisonous mushroom based on it season of growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "Dr69fyXEvEmp",
    "outputId": "60e65f7a-9bbc-43bc-e07e-aaced3f7be1f"
   },
   "outputs": [],
   "source": [
    "poisonous_mushroom_type_count= poisonous_mushroom['season'].value_counts()  \n",
    "sns.barplot(x=poisonous_mushroom_type_count.index, y=poisonous_mushroom_type_count.values)\n",
    "plt.ylabel('Number of data', fontsize=9)\n",
    "plt.xlabel('Season', fontsize=9)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();\n",
    "df_primary_data['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6ABe5jJvEmp"
   },
   "source": [
    "### Q. Which seasons do poisonous mushrooms grow typically?\n",
    "### A. Over Summer and Autumn most of the poisonous mushrooms typically grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuKMpnGQvEmp"
   },
   "source": [
    "# Conclusion : \n",
    "\n",
    "### Q. How many mushrooms are poisonous?\n",
    "### A. 96 mushrooms are poisonous and 77 are edible\n",
    "### Q. Which family contains most poisonous mushrooms?\n",
    "### A. Tricholoma, mushroom family has highest number of poisonous mushrooms.\n",
    "### Q. Do all poisonous mushrooms have a ring?\n",
    "### A. No, all the poisonous mushroom does not have ring, only 70 out of 96 poisonous mushroom have ring.\n",
    "### Q. Which seasons do poisonous mushrooms grow typically?\n",
    "### A. Over Summer and Autumn most of the poisonous mushrooms typically grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwWK7GNQvEmp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
