{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b49d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf3a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_files = os.listdir(self.root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.img_files[index])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = 1 if \"parasite\" in self.root_dir else 0\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3fa948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CellImageClassifier, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.pool3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c238770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7af6aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.3294, Accuracy = 1.0000\n",
      "Epoch 2: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 3: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 4: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 5: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 6: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 7: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 8: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 9: Loss = 0.3133, Accuracy = 1.0000\n",
      "Epoch 10: Loss = 0.3133, Accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set up the root directory for the image data\n",
    "root_dir = \"E:/Document/M-Tech/M-Tech Class/2nd Trimester/Assignment/AI Lab/Assignment 7/Datasets/ch22m503/\"\n",
    "\n",
    "# Define the image transformations to apply\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create the dataset objects for the two classes\n",
    "uninfected_data = CellImageDataset(root_dir + \"/uninfected\", transform=transform)\n",
    "parasite_data = CellImageDataset(root_dir + \"/parasite\", transform=transform)\n",
    "\n",
    "# Determine the number of samples in each class\n",
    "num_uninfected = len(uninfected_data)\n",
    "num_parasite = len(parasite_data)\n",
    "\n",
    "# Determine the class weights to use for weighted sampling\n",
    "class_weights = torch.Tensor([1.0, num_uninfected / num_parasite])\n",
    "\n",
    "# Create the weighted sampler for the training data\n",
    "sampler = torch.utils.data.WeightedRandomSampler(class_weights, num_samples=num_parasite+num_uninfected, replacement=True)\n",
    "\n",
    "# Combine the two datasets into one and create the data loader using the weighted sampler\n",
    "train_data = torch.utils.data.ConcatDataset([uninfected_data, parasite_data])\n",
    "train_loader = DataLoader(train_data, batch_size=32, sampler=sampler)\n",
    "\n",
    "# Define the CNN model and optimizer\n",
    "model = CellImageClassifier()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function with class weighting\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "for epoch in range(10):\n",
    "    epoch_loss, epoch_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(\"Epoch {}: Loss = {:.4f}, Accuracy = {:.4f}\".format(epoch+1, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983ba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feac6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bdf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c00516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b31e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be775373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a460a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'E:\\\\Document\\\\M-Tech\\\\M-Tech Class\\x02nd Trimester\\\\Assignment\\\\AI Lab\\\\Assignment 7\\\\Datasets\\\\ch22m503Parasitized\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m dataset \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m#Many ways to handle data, you can use pandas. Here, we are using a list format.  \u001b[39;00m\n\u001b[0;32m     36\u001b[0m label \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m#Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m parasitized_images \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mParasitized\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parasitized_images):    \u001b[38;5;66;03m#Remember enumerate method adds a counter and returns the enumerate object\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (image_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'E:\\\\Document\\\\M-Tech\\\\M-Tech Class\\x02nd Trimester\\\\Assignment\\\\AI Lab\\\\Assignment 7\\\\Datasets\\\\ch22m503Parasitized\\\\'"
     ]
    }
   ],
   "source": [
    "# https://youtu.be/HXzz87WVm6c\n",
    "\"\"\"\n",
    "Dataset from: https://lhncbc.nlm.nih.gov/publication/pub9932\n",
    "\n",
    "Binary problem: Question is: Is the cell in the image infected/parasited? \n",
    "If yes, probability is close to 1. \n",
    "If no, the probablility is close to 0. (uninfected)\n",
    "\n",
    "This is because we added label 1 to parasited images. \n",
    "In summary, probability result close to 1 reflects infected (parasited) image \n",
    "and close to 0 reflects uninfected image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras import backend as K\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#Test GPU\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "image_directory = 'E:\\Document\\M-Tech\\M-Tech Class\\2nd Trimester\\Assignment\\AI Lab\\Assignment 7\\Datasets\\ch22m503'\n",
    "SIZE = 150\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "\n",
    "parasitized_images = os.listdir(image_directory + 'Parasitized\\\\')\n",
    "for i, image_name in enumerate(parasitized_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "    \n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Parasitized\\\\' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)\n",
    "        \n",
    "uninfected_images = os.listdir(image_directory + 'Uninfected\\\\')\n",
    "for i, image_name in enumerate(uninfected_images):\n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Uninfected\\\\' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "        \n",
    "dataset = np.array(dataset)\n",
    "label = np.array(label)\n",
    "print(\"Dataset size is \", dataset.shape)\n",
    "print(\"Label size is \", label.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 0)\n",
    "print(\"Train size is \", X_train.shape)\n",
    "print(\"Test size is \", X_test.shape)\n",
    "\n",
    "from keras.utils import normalize\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "###Define the model\n",
    "\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=INPUT_SHAPE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer = 'he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer = 'he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))  \n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',             #also try adam\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary()) \n",
    "\n",
    "history = model.fit(X_train, \n",
    "                         y_train, \n",
    "                         batch_size = 64, \n",
    "                         verbose = 1, \n",
    "                         epochs = 100,      \n",
    "                         validation_data=(X_test,y_test),\n",
    "                         shuffle = False\n",
    "                     )\n",
    "\n",
    "\n",
    "model.save('E:\\Document\\M-Tech\\M-Tech Class\\2nd Trimester\\Assignment\\AI Lab\\Assignment 7\\Datasets\\ch22m503\\malaria_model_100epochs.h5')  \n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "n=23  #Select the index of image to be loaded for testing\n",
    "img = X_test[n]\n",
    "plt.imshow(img)\n",
    "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "print(\"The prediction for this image is: \", model.predict(input_img))\n",
    "print(\"The actual label for this image is: \", y_test[n])\n",
    "\n",
    "\n",
    "#We can load the trained model, so we don't have to train again for 300 epochs!\n",
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('models/malaria_model_100epochs.h5')\n",
    "\n",
    "#For 300 epochs, giving 82.5% accuracy\n",
    "\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "mythreshold=0.885\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (model.predict(X_test)>= mythreshold).astype(int)\n",
    "cm=confusion_matrix(y_test, y_pred)  \n",
    "print(cm)\n",
    "\n",
    "\n",
    "#ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "y_preds = model.predict(X_test).ravel()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "i = np.arange(len(tpr)) \n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'thresholds' : pd.Series(thresholds, index=i)})\n",
    "ideal_roc_thresh = roc.iloc[(roc.tf-0).abs().argsort()[:1]]  #Locate the point where the value is close to 0\n",
    "print(\"Ideal threshold is: \", ideal_roc_thresh['thresholds']) \n",
    "\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_value = auc(fpr, tpr)\n",
    "print(\"Area under curve, AUC = \", auc_value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11920463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
